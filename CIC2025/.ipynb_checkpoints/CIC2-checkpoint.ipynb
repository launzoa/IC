{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce84d20a-92e5-42f1-931e-34bd892993a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10.299629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m pred_x = odeint(func, torch.tensor(initial_state, dtype=torch.float32), t_train)\n\u001b[32m     75\u001b[39m loss = loss_fn(pred_x, x_train)  \u001b[38;5;66;03m# diferença entre rede e solução exata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m optimizer.step()\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m200\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/IC/venvCIC/lib/python3.13/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/IC/venvCIC/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/IC/venvCIC/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/IC/venvCIC/lib/python3.13/site-packages/torch/autograd/function.py:296\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBackwardCFunction\u001b[39;00m(_C._FunctionBase, FunctionCtx, _HookMixin):\n\u001b[32m    292\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m    297\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[32m    299\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[32m    301\u001b[39m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# ====================================================\n",
    "# 1. Definição do Sistema Dinâmico - Oscilador Harmônico Amortecido\n",
    "# ====================================================\n",
    "\n",
    "# Parâmetros físicos do sistema\n",
    "damping_ratio = 0.1        # ζ : coeficiente de amortecimento (adimensional)\n",
    "natural_frequency = 2.0    # ω_n : frequência natural (rad/s)\n",
    "t_max = 20.0               # tempo total de simulação\n",
    "t_eval = np.linspace(0, t_max, 200)  # pontos de amostragem\n",
    "\n",
    "def true_dynamics(t, state):\n",
    "    \"\"\"\n",
    "    Campo de vetores do oscilador harmônico amortecido.\n",
    "    Sistema de 1ª ordem equivalente à EDO de 2ª ordem:\n",
    "        y'' + 2ζω_n y' + ω_n² y = 0\n",
    "    state[0] = posição (x1 = y)\n",
    "    state[1] = velocidade (x2 = y')\n",
    "    \"\"\"\n",
    "    dx1 = state[1]\n",
    "    dx2 = -2 * damping_ratio * natural_frequency * state[1] - natural_frequency**2 * state[0]\n",
    "    return [dx1, dx2]\n",
    "\n",
    "# Condições iniciais (posição inicial = 1, velocidade inicial = 0)\n",
    "initial_state = [1.0, 0.0]\n",
    "\n",
    "# Solução \"ground truth\" obtida com solver clássico de alta precisão (RK45)\n",
    "sol = solve_ivp(true_dynamics, [0, t_max], initial_state,\n",
    "                t_eval=t_eval, method='RK45', rtol=1e-10, atol=1e-12)\n",
    "ground_truth = sol.y.T\n",
    "t_train = torch.tensor(t_eval, dtype=torch.float32)\n",
    "\n",
    "# ====================================================\n",
    "# 2. Definição da Rede Neural (Função f(x) aproximada)\n",
    "# ====================================================\n",
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    Rede neural que aproxima a função f(x):\n",
    "        f([x1, x2]) = [dx1/dt, dx2/dt]\n",
    "    Estrutura: 2 -> 32 -> 2 (com ativação Tanh intermediária).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 32),   # entrada: posição e velocidade\n",
    "            nn.Tanh(),          # não-linearidade\n",
    "            nn.Linear(32, 2)    # saída: derivadas dx1 e dx2\n",
    "        )\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "func = ODEFunc()\n",
    "\n",
    "# ====================================================\n",
    "# 3. Treinamento do Neural ODE\n",
    "# ====================================================\n",
    "optimizer = optim.Adam(func.parameters(), lr=0.001)  # otimizador Adam\n",
    "loss_fn = nn.MSELoss()  # função de custo: erro quadrático médio\n",
    "\n",
    "# Dados de treinamento = solução ground truth\n",
    "x_train = torch.tensor(ground_truth, dtype=torch.float32)\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    # Resolver ODE neural\n",
    "    pred_x = odeint(func, torch.tensor(initial_state, dtype=torch.float32), t_train)\n",
    "    loss = loss_fn(pred_x, x_train)  # diferença entre rede e solução exata\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# ====================================================\n",
    "# 4. Teste de Estabilidade\n",
    "# ====================================================\n",
    "\n",
    "# Criar uma perturbação inicial (ε pequeno)\n",
    "perturbation = np.array([0.01, -0.01])\n",
    "initial_state_perturb = [initial_state[0] + perturbation[0],\n",
    "                         initial_state[1] + perturbation[1]]\n",
    "\n",
    "# Solução ground truth com perturbação\n",
    "sol_perturb = solve_ivp(true_dynamics, [0, t_max], initial_state_perturb,\n",
    "                        t_eval=t_eval, method='RK45', rtol=1e-10, atol=1e-12)\n",
    "ground_truth_perturb = sol_perturb.y.T\n",
    "\n",
    "# Neural ODE original e perturbado\n",
    "pred_x = odeint(func, torch.tensor(initial_state, dtype=torch.float32), t_train).detach().numpy()\n",
    "pred_x_perturb = odeint(func, torch.tensor(initial_state_perturb, dtype=torch.float32), t_train).detach().numpy()\n",
    "\n",
    "# Função para calcular a norma do erro ||x - x_p||\n",
    "def error_norm(sol1, sol2):\n",
    "    return np.linalg.norm(sol1 - sol2, axis=1)\n",
    "\n",
    "error_true = error_norm(ground_truth, ground_truth_perturb)\n",
    "error_neural = error_norm(pred_x, pred_x_perturb)\n",
    "\n",
    "# ====================================================\n",
    "# 5. Visualizações\n",
    "# ====================================================\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# (a) Diagramas de fase\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ground_truth[:, 0], ground_truth[:, 1], 'k-', label=\"Ground Truth (RK45)\")\n",
    "plt.plot(pred_x[:, 0], pred_x[:, 1], 'r--', label=\"Neural ODE\")\n",
    "plt.xlabel(\"x1 (posição)\")\n",
    "plt.ylabel(\"x2 (velocidade)\")\n",
    "plt.title(\"Diagrama de Fase: RK45 vs Neural ODE\")\n",
    "plt.legend()\n",
    "\n",
    "# (b) Normas de erro\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_eval, error_true, 'k-', label=\"Erro Ground Truth (perturbado)\")\n",
    "plt.plot(t_eval, error_neural, 'r--', label=\"Erro Neural ODE (perturbado)\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"||x - x_p||\")\n",
    "plt.title(\"Análise de Estabilidade (Lyapunov Empírico)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ep5/cic11.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
